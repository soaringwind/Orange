# MTCNN算法与代码理解

在目标检测中有多种算法，如yolo，SSD等等，而在人脸检测中MTCNN与经典的Adaboost算法用的较多。两个算法纯手动实现，对比来看，MTCNN的效果要更好一些，可以算得上是又快又好。下面介绍两种算法的整体流程。
## MTCNN算法
首先介绍MTCNN的网络结构，MTCNN实际上是由三个网络构成的，分别是P-net(Proposal network)，R-net(Refine network)和O-net(Output network)。三个网络输入图片的大小不同，但是可以同时训练。第一层网络输入图片的大小为12，用来生成多个目标区域，第二层网络输入图片的大小为24，是将第一层网络给出的目标区域进行更精细的判断，第三层网络输入的图片大小为48，是将第二层网络给出的目标区域进行更准确的判断，并给出图像特征点的坐标信息（因为此时拿到的区域可以认为是较为准确的区域，可以找到感兴趣的特征点信息）。下面是MTCNN三层网络的结构示意图。

![image](https://user-images.githubusercontent.com/26198992/177107220-5450e4f3-45bb-42f7-b118-e38a7bf5ea43.png)

## 训练过程

1. 准备数据
做神经网络训练的时候，最重要的就是数据，幸运的是celeb准备了足够多的数据，并且是附带有预测框以及关键点信息的数据。我们只需要去读取该数据集的信息，就可以准备我们的训练集。准备数据的时候，同样分为三份数据，截取出人脸框，将区域图像尺寸大小转为12，24以及48，但是需要对框进行一定程度的偏移，这样可以准备出负样本（iou<0.3），部分样本（iou<0.65）以及正样本（iou>=0.65）这三类数据，比例大概为3：1：1。
2. 损失函数
准备好数据之后，就要准备开始网络的训练。由于MTCNN是由分类问题（判断是否为人脸）和回归问题（计算该框偏移量）的组合，因此每个网络的损失函数也是由两部分构成。第一部分，分类问题，使用的是交叉熵损失或者BCE损失，第二部分使用的是MSE损失，计算损失的时候会将两部分相加得到该次损失。网络的输出层，可以看到都为10，这是因为前面2个为人脸结果，中间4个为框的四个点坐标，最后4个是四个特征点的信息。我这里不需要得到特征点的信息，因此最后输出层统一为6。
3. 训练网络
这里正常情况下是先训练好P-net，再由P-net的结果去训练R-net，但是这里简便直接将之前的数据拿来训练。同时训练三个网络，由于网络结构较小，因此训练速度是较快的，但是由于是两类损失求和，因此训练的loss最好在0.001以下，效果会比较好。
## 测试过程

测试过程实际上就是检测过程，一张任意尺寸的图片，首先构建图像金字塔，以2为步长，每次取尺寸为12的区域来构建P-net的输入。人脸尺寸的不统一，因此需要构建多尺寸的图像金字塔，每次对图像进行一定尺度的缩放，直到图片尺寸小于12为止。图像金字塔的构建如下图。

![img](https://user-images.githubusercontent.com/26198992/177116294-a8de539b-f2b4-408c-82c2-110ad8b650c9.png)

将所有图片stack之后放入到第一层P-net中，得到多个感兴趣的区域，之后进行非极大值抑制来减少重复区域。
将P-net网络得到的结果，根据缩放尺度以及补偿值，得到其在原图的坐标，将该区域对应的照片取出之后，进行尺寸缩放到24，送入到R-net网络中，之后类似，仍需要做非极大值抑制。
最后将R-net网络得到的结果，得到原图坐标，取出该区域，送入到O-net网络中，得到最后的结果。整体流程如下图所示。

![image](https://user-images.githubusercontent.com/26198992/177117841-caa09631-eb35-470f-9fa6-e5c347a89afd.png)

## 小工具
1. iou计算工具：计算两个区域的交并集的比例，其实就是找到两个区域的相交区域的坐标，之后计算即可。
2. nms：非极大值抑制这个已经用过很多次了，这里就是拿到得分最高的，然后计算iou，把iou高于0.3的都抑制掉，再找下一个得分最高即可。
